{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33477a68",
   "metadata": {},
   "source": [
    "# Noize net\n",
    "The following notebook contains the code used for both an LSTM and RNN used for investigating the feasibility of music generation using an RNN. \n",
    "\n",
    "Note that berevity of code both the RNN and LSTM are implimented in one code base. This does mean that the code is somewhat more complicated and necessitates the use of \"LSTMBool\" which defines throughout the code if operations will take place for an LSTM or RNN. \n",
    "\n",
    "Note to reader: This notebook is a refactor of the main python file noizenet.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284f8e1",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "First we must import all the necessary dependencies.\n",
    "\n",
    "Then check if we can run on a GPU.\n",
    "\n",
    "Additionally we can define the scaler for scaling input data and inverting a scale on output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035b06ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing on GPU.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib   \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import librosa as lib\n",
    "import os\n",
    "import soundfile as sf #For writing\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import utils\n",
    "import librosa.display\n",
    "import noise\n",
    "\n",
    "\n",
    "# torch.autograd.detect_anomaly(True) #Check for errors and return a stack trace. (Used to debug nan loss)\n",
    "\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "#Print if we are able to use a GPU\n",
    "if(train_on_gpu):\n",
    "    print('Processing on GPU.')\n",
    "else:\n",
    "    print('No GPU available.')\n",
    "\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60baf26",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "Bellow we define some functions that will be used later in this work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4a6ac",
   "metadata": {},
   "source": [
    "## Time to FT\n",
    "Convert an array of time/amplitude values to frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d33a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_fft(time_domain):\n",
    "    # Compute FFT\n",
    "    fft = np.fft.fft(time_domain)\n",
    "    # Concatinate real and imaginary values\n",
    "    new_input = np.concatenate((np.real(fft_block), np.imag(fft_block)))\n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3affcc69",
   "metadata": {},
   "source": [
    "## FT to Time\n",
    "Convert an array of fourier values to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4bd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_to_time(ft_domain):\n",
    "    # check if the input can be divinded in two and assign as needed\n",
    "    if ft_domain.shape[0] % 2 == 0:\n",
    "        num_elems = (int)(blocks_ft_domain.shape[0] / 2)\n",
    "    else:\n",
    "        ft_domain = ft_domain[0:-1]\n",
    "        num_elems = (int)(ft_domain.shape[0] / 2)\n",
    "    # Get real part \n",
    "    real = ft_domain[0:num_elems]\n",
    "    # Get imaginary part\n",
    "    imag = ft_domain[num_elems:]\n",
    "    # Recompose real and Im parts\n",
    "    composition = real + 1.0j * imag\n",
    "    # ifft back to time domain\n",
    "    time = np.fft.ifft(composition)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662337a",
   "metadata": {},
   "source": [
    "## Batching function \n",
    "This function is rarely used in this work but some testing was done with batches to check their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96ebbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    #Note this code was adapted fome Udacity course on machine learning. \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough elements to fill a batch\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02a11b",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "Then we can define the Model using the pytorch class definition.\n",
    "\n",
    "The model contains the following layers:\n",
    "* LSM or RNN layer \n",
    "* Dropout Layer\n",
    "* Fully connected layer for translating oyutput\n",
    "\n",
    "After defining the layers in the model we can define a forward function used for the formward pass during training and prediction. \n",
    "* Given c0 (Or None for RNN), the hidden state and some imput x.\n",
    "* Pass the hidden state and input through the RNN layer and recieve some output\n",
    "* Selectively choose values to drop (Note this only happens for hidden layers > 1)\n",
    "* Pass the result through the fully connected layer to obtain the final result\n",
    "* Return the prediction and hidden states\n",
    "\n",
    "Additionally we define a helper funcation to generate the hidden states for an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ec3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoizeNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob):\n",
    "        super(NoizeNet, self).__init__()\n",
    "\n",
    "        self.LSTMBool = LSTMBool  \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "\n",
    "        #TODO: Test RNN vs LSTM\n",
    "        if LSTMBool:\n",
    "            self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=dropout_prob,batch_first=True)\n",
    "            self.hidden = (torch.zeros(1,1,self.hidden_dim).cuda(), torch.zeros(1,1,self.hidden_dim).cuda()) #We need a tuple for a LSTM\n",
    "        else:\n",
    "            # define an RNN with specified parameters\n",
    "            # batch_first means that the first dim of the input and output will be the batch_size\n",
    "            self.rnn = nn.RNN(input_size, hidden_dim, n_layers, dropout=dropout_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # last, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, c0=None):\n",
    "        batch_size = x.size(0)\n",
    "        if (train_on_gpu):\n",
    "            x.cuda()\n",
    "        else:\n",
    "            x.cpu()\n",
    "        \n",
    "\n",
    "        if self.LSTMBool:\n",
    "            h_0 = hidden\n",
    "            c_0 = c0\n",
    "            #Rename variables as convention\n",
    "            state = (h_0, c_0)\n",
    "            # Propagate input through LSTM\n",
    "            r_out, (hn, cn) = self.lstm(x, state) #lstm with input, hidden, and internal state\n",
    "            hidden = hn\n",
    "        else:\n",
    "            # get RNN outputs\n",
    "            r_out, hidden = self.rnn(x , hidden)\n",
    "\n",
    "        r_out = self.dropout(r_out) #Dropout\n",
    "\n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        if (train_on_gpu):\n",
    "            hidden.cuda()\n",
    "            \n",
    "        r_out = r_out.view(-1, self.hidden_dim)\n",
    "\n",
    "        # get final output\n",
    "        output = self.fc(r_out)\n",
    "\n",
    "        if self.LSTMBool:\n",
    "            return output, (hidden, cn)\n",
    "        else:\n",
    "            return output, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        #THIS CODE IS FROM UDACITY\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = torch.autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)).cuda() #hidden state\n",
    "            c0 = torch.autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)).cuda() #internal state\n",
    "        else:\n",
    "            hidden = torch.autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)).cuda() #hidden state\n",
    "            c0 = torch.autograd.Variable(torch.zeros(self.num_layers, 1, self.hidden_dim)).cuda() #internal state\n",
    "\n",
    "        return hidden, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef2961",
   "metadata": {},
   "source": [
    "# Training\n",
    "We can then define the training function. This function takes in the model, n_steps which is a bit of a misnoma as it is used as the number of full \"frame\" steps that will complete the impute sequence. An array of genreTracks which are translated into file names. Using AUDIO_DIR and file names we can read in the input songs. Step size is used to determine how large of a step to take in the training data. Duration is the duration of the input song to read in. Number of tracks is the number of songs that we will train on. Clip is the value of the griadient clipping. \n",
    "\n",
    "In this work we vary the training procedue among trails. However the procedure remains unchanged and is as follows:\n",
    "* Ensure model is in training mode\n",
    "* load validation data and perform and needed transformations/scaling\n",
    "* Initialize hidden layers\n",
    "* Loop over a number of input tracks\n",
    "* At each loop iteration\n",
    "    * Load and transform input training data\n",
    "    * Define the needed variables\n",
    "    * Check if input data contains a NaN value\n",
    "    * For some number of steps that was calculated earlier\n",
    "        * For some number of epochs \n",
    "            * Select some data\n",
    "            * Ensure data has a shift in input and target\n",
    "            * Convert to tensors and give the input a batch size dimension of one\n",
    "            * Move data to GPU if we can\n",
    "            * Get a prediction frome the LSTM or RNN\n",
    "            * Detach hidden states from history\n",
    "            * Zero gradients\n",
    "            * Calculate loss and add it to the plot array\n",
    "            * Perform backward step\n",
    "            * Clip gradients\n",
    "            * Take an optimizer step to optimize weights\n",
    "            * Perform validation (I will not walk through this as it is the same procedure in eval mode)\n",
    "    * Clean memory if the model is large\n",
    "* Plot losses\n",
    "* Return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0b4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(noizeNet, n_steps, AUDIO_DIR, genreTracks, LSTM ,step_size=1, duration=5, numberOfTracks=1, clip=5, fft_bool=False):\n",
    "    fileCount = 0 #Used for displaying the file that is currently being trained on\n",
    "    noizeNet.train() #Set the model to training mode\n",
    "    lossArr = [] #Array used to plot loss over time\n",
    "    val_losses = []\n",
    "    if(train_on_gpu):\n",
    "        noizeNet.cuda() #Move the model to GPU if available\n",
    "\n",
    "    val_file = utils.get_audio_path(AUDIO_DIR, genreTracks[-1])\n",
    "    val_data, sr = lib.load(val_file, mono=True, duration = duration)\n",
    "    if fft_bool:\n",
    "        print(\"We are training on FFT data\")\n",
    "        val_data = time_to_fft(val_data)\n",
    "    val_data = scaler.fit_transform(val_data.reshape(-1,1)) #Scale data\n",
    "    hidden = None\n",
    "    c0 = None\n",
    "    hidden, c0 = noizeNet.init_hidden(1)\n",
    "    #Loop over all the files in our filtered list\n",
    "    for id in genreTracks: \n",
    "        #Stop training after n files\n",
    "        fileCount+=1\n",
    "        if(fileCount > numberOfTracks):\n",
    "            break \n",
    "\n",
    "        filename = utils.get_audio_path(AUDIO_DIR, id) #Get the actual path to the file from the id\n",
    "        \n",
    "        fileData, sr = lib.load(filename, mono=True, duration = duration)\n",
    "        fileData = fileData.reshape(-1,1)\n",
    "        if fft_bool:\n",
    "            fileData = time_to_fft(fileData)\n",
    "        fileData = scaler.fit_transform(fileData)\n",
    "\n",
    "        data = fileData\n",
    "        batch_size = (int)(duration*sr/n_steps) #Find the size of the window that slides across the input song\n",
    "        number_of_steps = (int)(duration*sr)-batch_size #Assumes step size of one as a larger step size produced poor results\n",
    "\n",
    "        if(np.isnan(np.sum(fileData))):\n",
    "            print(\"NAN ON FILE:\\t\", filename)\n",
    "            break\n",
    "        \n",
    "        for batch_i in (range(0,number_of_steps, step_size)):\n",
    "            for e in range(0,1):\n",
    "                # for batch_i in (range(0,number_of_steps, step_size)):\n",
    "                # if LSTM:\n",
    "                #     (hidden, c0) = noizeNet.init_hidden(batch_size)\n",
    "                # for x, y in get_batches(fileData, 500, 50):\n",
    "                \n",
    "                # defining the training data\n",
    "                data = fileData[(batch_i): batch_size + batch_i]\n",
    "                # data = np.resize(data,((batch_size), 1)) \n",
    "                # data = data.reshape(batch_size,1)  \n",
    "                # data = data.reshape(len(data),1)    \n",
    "                x = data[:-1] #Select all but the last element in the input data\n",
    "                y = data[1:] #Select all but the first element in the input data. Essentially a forward shift in time\n",
    "\n",
    "                # convert data into Tensors\n",
    "                x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "                y_tensor = torch.Tensor(y)\n",
    "                if(train_on_gpu):\n",
    "                        x_tensor, y_tensor = x_tensor.cuda(), y_tensor.cuda()\n",
    "                \n",
    "                if LSTM:\n",
    "                    prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0) #LSTM!\n",
    "                else:\n",
    "                    # outputs from the rnn\n",
    "                    prediction, hidden = noizeNet(x_tensor, hidden)\n",
    "                \n",
    "                ## Representing Memory ##\n",
    "                # make a new variable for hidden and detach the hidden state from its history\n",
    "                # this way, we don't backpropagate through the entire history\n",
    "                hidden = hidden.data\n",
    "\n",
    "                if LSTM:\n",
    "                    c0 = c0.data #LSTM!\n",
    "\n",
    "\n",
    "                # zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # calculate the loss\n",
    "                \n",
    "                loss = criterion(prediction, y_tensor)\n",
    "                # if(np.isnan(loss)):\n",
    "                #     break\n",
    "                lossArr.append(loss.item())\n",
    "                # perform backprop and update weights\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_value_(noizeNet.parameters(), clip) #Clip gradient\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                #Validation step\n",
    "                if(int((batch_i)) % 1000 == 0):\n",
    "                    if LSTM:\n",
    "                        (val_h, val_c) = noizeNet.init_hidden(1)\n",
    "                    else:\n",
    "                        val_h = None\n",
    "\n",
    "                    \n",
    "                    noizeNet.eval()\n",
    "                    val_x = val_data[:-1]\n",
    "                    val_y = val_data[1:]\n",
    "\n",
    "                    \n",
    "\n",
    "                    for e in range(0,1):\n",
    "                        val_x, val_y = torch.from_numpy(val_x), torch.from_numpy(val_y)\n",
    "                        \n",
    "                        \n",
    "                        inputs, targets = val_x.reshape(-1,1).unsqueeze(0), val_y.reshape(-1,1)\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        if LSTM:\n",
    "                            output, (val_h, val_c) = noizeNet(inputs, val_h, val_c)\n",
    "                        else:\n",
    "                            output, val_h = noizeNet(inputs, val_h)\n",
    "                        val_loss  = criterion(output, targets)\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                        if LSTM:\n",
    "                            val_h = val_h.data\n",
    "                            val_c = val_c.data\n",
    "                        else:\n",
    "                            val_h = val_h.data\n",
    "\n",
    "\n",
    "                    print(\"Training Progress:\\t\", round((fileCount*e/(number_of_steps*numberOfTracks))*100, 2), \"%\"  , sep=\"\")\n",
    "                    print(\"P:\\t\", prediction.cpu().data.numpy().flatten()[-5:],\"\\nY:\\t\", y[-5:].flatten() ,\"\\nX:\\t\" , x[-5:].flatten() ,sep=\"\")\n",
    "                    print('Training Loss: ', loss.item(), \"Validation loss: \", val_loss.item() , \"\\t num:\", batch_i, \"\\t File:\", fileCount)\n",
    "                    noizeNet.train() # reset to train mode after validation\n",
    "            \n",
    "            #Clean unused variables to ensure memory is kept free\n",
    "            # del prediction\n",
    "            # del x_tensor\n",
    "            # del y_tensor\n",
    "            # del data\n",
    "            # del x\n",
    "            # del y\n",
    "            # gc.collect()\n",
    "        fig, ax = plt.subplots(nrows=2)\n",
    "        ax[0].plot(lossArr)\n",
    "        ax[0].set(title=\"Training loss\", ylabel=\"Loss\", xlabel=\"Epochs\")\n",
    "        ax[1].plot(val_losses)\n",
    "        ax[1].set(title=\"Validation loss\", ylabel=\"Loss\", xlabel=\"Epochs\")\n",
    "        plt.show()\n",
    "    return noizeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f39f4",
   "metadata": {},
   "source": [
    "# Prediction and Music generation\n",
    "\n",
    "Then we define the prediction function. \n",
    "\n",
    "In this work we have two prediction schemes that produce equivalent results. I will explain them independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b7918",
   "metadata": {},
   "source": [
    "## Prediction scheme 1 \n",
    "This prediction sheme functions as follows\n",
    "* Recieve the model, prediction seed track, duration of seed, prediction duration and LSTM bool\n",
    "* Read in the seed data and perform any needed scaling/transformation\n",
    "* Select some range of the seed and obtain a prediction from the model\n",
    "* Record the last element in the prediction\n",
    "* Append this predicted value to a music array that will become the generated song\n",
    "* Shift the data window to exclude an early data point and append the predicted value\n",
    "* Repeat this procedure until we have predicted enough points for the desired music output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36c661a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(noizeNet, genreTrack ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval()\n",
    "    hidden = None\n",
    "    \n",
    "    if LSTMBool:\n",
    "        c0 = None\n",
    "        (hidden, c0) = noizeNet.init_hidden(1)\n",
    "    filePath = utils.get_audio_path(AUDIO_DIR, genreTrack) #Get the actual path to the file from the id\n",
    "    y, sr = lib.load(filePath, mono=True, duration = duration)\n",
    "    # y = time_to_fft(y)\n",
    "    y = scaler.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "    data = y\n",
    "    \n",
    "\n",
    "    # data = np.random.normal(-1,1,y.shape)\n",
    "    # data = genPerlin(np.linspace(0,1,y.size))\n",
    "    batch_size = (int)(sr*duration/n_steps)\n",
    "\n",
    "\n",
    "\n",
    "    number_of_steps = (int)(sr*predictDuration/step_size)\n",
    "\n",
    "    music = []\n",
    "    next = data[batch_size-1]\n",
    "\n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/predictionSeed.wav', np.append( data[step_size: batch_size], next), sr,format=\"WAV\")\n",
    "    print(\"BATCH SIZE:\", batch_size ,sep=\"\\t\")\n",
    "    print(\"NUMBER OF STEPS:\", number_of_steps , sep=\"\\t\")\n",
    "    # data = data[0: batch_size-1]\n",
    "    hidden = None\n",
    "    c0 = None\n",
    "    hidden, c0 = noizeNet.init_hidden(1)\n",
    "    for batch_i in (range(0, number_of_steps)):\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda()\n",
    "        data = data[batch_i: batch_size-1 + batch_i]\n",
    "        data = np.append(data,next)\n",
    "        x = data.reshape(-1,1)\n",
    "\n",
    "        print(data.shape)\n",
    "        if(np.isnan(np.sum(data))):\n",
    "            print(\"data contains NAN\", data)\n",
    "\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda()\n",
    "\n",
    "        if(LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0)\n",
    "            c0 = c0.data\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden)\n",
    "        hidden = hidden.data\n",
    "        \n",
    "        \n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-1])\n",
    "        next = prediction.cpu().data.numpy().flatten()[-1]\n",
    "        \n",
    "        if(int((batch_i)) % 100 == 0):\n",
    "            # fig, ax = plt.subplots(nrows=2)\n",
    "            # ax[0].plot(prediction.cpu().data.numpy())\n",
    "            # ax[1].plot(scaler.inverse_transform(prediction.cpu().data.numpy()))\n",
    "            # print(np.average(prediction.cpu().data.numpy()))\n",
    "            # print(prediction.cpu().data.numpy().flatten()[-1:])\n",
    "            # print(x)\n",
    "            # ax[1].plot(fft_to_time(prediction.cpu().detach().numpy()))\n",
    "            # plt.show()\n",
    "            \n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    # print(fft_to_time(music))  \n",
    "    print(music)\n",
    "    music = scaler.inverse_transform(music.reshape(-1,1))\n",
    "    # music = prediction.cpu().detach().numpy()\n",
    "    # music = abs(fft_to_time(music))#.astype('float32')\n",
    "    # print(music)\n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/outputSoundFile.wav', (music), 22050,format=\"WAV\")\n",
    "    time_steps = np.linspace(0, len(music), len(music))\n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=0.1)\n",
    "    plt.show()\n",
    "    return music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273f1cd",
   "metadata": {},
   "source": [
    "## Prediction Scheme 2\n",
    "This scheme is similar to the previous one, however, more efficient as we process the seed element by elements and then pass the hidden states to a new function that will begin predicting new values from the last predicted value from the seed and hidden states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d132df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(noizeNet, genreTrack ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval()\n",
    "\n",
    "    if LSTMBool:\n",
    "        hidden, c0 = noizeNet.init_hidden(1)\n",
    "    else:\n",
    "        hidden = None\n",
    "    filePath = utils.get_audio_path(AUDIO_DIR, genreTrack) #Get the actual path to the file from the id\n",
    "    y, sr = lib.load(filePath, mono=True, duration = duration)\n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/Predict2Seed.wav', y, 22050,format=\"WAV\")\n",
    "    y = scaler.fit_transform(y.reshape(-1,1))\n",
    " \n",
    "    batch_size = (int)(sr*duration/n_steps)\n",
    "\n",
    "    number_of_steps = (int)(sr*duration/step_size)\n",
    "    music = []\n",
    "    for batch_i in (range(0, number_of_steps)):\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda()\n",
    "        data = y[batch_i]\n",
    "        data = data.reshape(-1,1)\n",
    "        x = data\n",
    "\n",
    "\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda()\n",
    "\n",
    "        if(LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0)\n",
    "            c0 = c0.data\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden)\n",
    "        hidden = hidden.data\n",
    "        \n",
    "        print(prediction.cpu().data.numpy().shape)\n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-step_size:])\n",
    "        if(int((batch_i)) % 1000 == 0):\n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/outputSoundFilePredict2.wav', (music), 22050,format=\"WAV\")\n",
    "    time_steps = np.linspace(0, len(music), len(music))\n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=1)\n",
    "    plt.show()\n",
    "    if LSTMBool:\n",
    "        return music, (hidden, c0)\n",
    "    else: \n",
    "        return music, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e07f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict3(noizeNet, seeded ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1, hidden = None, c0 = None):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval()\n",
    "    number_of_steps = (int)(22050*predictDuration/step_size)\n",
    "    music = []\n",
    "\n",
    "    for batch_i in (range(0, number_of_steps)):\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda()\n",
    "        seeded = np.array([seeded]).reshape(-1,1)\n",
    "        data = seeded\n",
    "        if x - seeded[0] < 0.001:\n",
    "            data = seeded*np.random.normal(-1,1)\n",
    "        data = data.reshape(-1,1)\n",
    "        x = data\n",
    "        \n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "        print(\"x_tensor\",x_tensor)\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda()\n",
    "\n",
    "        if (LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0)\n",
    "            c0 = c0.data\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden)\n",
    "        hidden = hidden.data\n",
    "        \n",
    "        seeded = prediction.cpu().data.numpy()[-1:step_size:]\n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-step_size:])\n",
    "        if(int((batch_i)) % 10000 == 0):\n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    # print(fft_to_time(music))  \n",
    "    # print(music)\n",
    "    music = scaler.inverse_transform(music)\n",
    "    # music = music.cpu().data.numpy().flatten()\n",
    "    # music = abs(fft_to_time(music))#.astype('float32')\n",
    "    print(music)\n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/outputSoundFilePrediction3.wav', (music), 22050,format=\"WAV\")\n",
    "    time_steps = np.linspace(0, len(music), len(music))\n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=1)\n",
    "    plt.show()\n",
    "    return music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fa53b",
   "metadata": {},
   "source": [
    "## Saving function\n",
    "A helper function used for naming a model by it's hyer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d921640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModelName(n_steps = 30, print_every = 5, step_size =  1, duration = 5, numberOfTracks = 1, clip = 5, LSTMBool=False, hidden_dim=50,n_layers=1):\n",
    "    return \"n_steps=\"+ str(n_steps) + \"__\" +\"print_every=\"+ str(print_every) + \"__\" +\"step_size=\"+ str(step_size) + \"__\" +\"duration=\"+ str(duration) + \"__\" +  \\\n",
    "    \"numberOfTracks=\"+ str(numberOfTracks) + \"__\" +  \"clip=\"+ str(clip) + \"__\" +  \"LSTMBool=\"+ str(LSTMBool) + \"hidden_dim=\"+ str(hidden_dim) + \"__\" +\"n_layers=\"+ str(n_layers) +\"__\"+ \"lr=\" + str(lr) + \".pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314be9",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "Before we start training we must declair the hyperparameters and instantiate the model.\n",
    "\n",
    "I believve all of these variables should be self explanatory. Note this is also where we choose if the miodel will be an LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3b53eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoizeNet(\n",
      "  (lstm): LSTM(1, 100, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# decide on hyperparameters\n",
    "# n_steps = 1\n",
    "input_size=1\n",
    "output_size=1\n",
    "hidden_dim=100\n",
    "n_layers=2\n",
    "LSTMBool = True\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# instantiate an RNN\n",
    "noizeNet = NoizeNet(input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob)\n",
    "print(noizeNet)\n",
    "\n",
    "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
    "criterion = nn.MSELoss()\n",
    "lr=0.0001\n",
    "# criterion = nn.L1Loss()\n",
    "# optimizer = torch.optim.Adam(noizeNet.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(noizeNet.parameters(),lr=lr)\n",
    "optimizer = torch.optim.Adadelta(noizeNet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0fe95",
   "metadata": {},
   "source": [
    "# Music filtering by genre\n",
    "\n",
    "Here we just filter all the data in FMA for some chosen genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ad7dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get metadata for fma dataset\n",
    "AUDIO_DIR = \"data/fma_small/\"\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "\n",
    "small = tracks['set', 'subset'] <= 'small'\n",
    "genre1 = tracks['track', 'genre_top'] == 'Instrumental'\n",
    "# genre2 = tracks['track', 'genre_top'] == 'Hip-Hop' #We can set multilpe genres bellow as (genre1 | genre2)\n",
    "genreTracks = list(tracks.loc[small & (genre1),('track', 'genre_top')].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddefd4",
   "metadata": {},
   "source": [
    "# Training variables\n",
    "\n",
    "Initialise the variables needed for training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set if we want to train new model or load and predict with saved model\n",
    "TRAIN = True\n",
    "\n",
    "n_steps = 30 #The number of full frame steps to be taken to complete training\n",
    "print_every = 5 \n",
    "step_size =  1 #The step size taken by each training frame \n",
    "duration = 1 #The duration of the training segment\n",
    "predictDuration = 1 #The duration of the predicted song is seconds\n",
    "numberOfTracks = 1 #The number of tracks to be trained on\n",
    "clip = 1 #Gradient clipping\n",
    "seedDuration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec44f7f",
   "metadata": {},
   "source": [
    "# Begin Training or load model and predict\n",
    "Then we can call our training and prediction functions as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/anaconda3/envs/myenv/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "/home/liam/anaconda3/envs/myenv/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress:\t0.0%\n",
      "P:\t[-0.04941618  0.00057781 -0.07336345 -0.06047632 -0.05029624]\n",
      "Y:\t[0.78485084 0.8130057  0.83501863 0.84733    0.8533735 ]\n",
      "X:\t[0.7577045  0.78485084 0.8130057  0.83501863 0.84733   ]\n",
      "Training Loss:  0.31421470642089844 Validation loss:  0.2561328113079071 \t num: 0 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.5456647  0.6161146  0.6258562  0.6115618  0.60452753]\n",
      "Y:\t[0.63663906 0.655915   0.67280006 0.6856986  0.6932378 ]\n",
      "X:\t[0.6161611  0.63663906 0.655915   0.67280006 0.6856986 ]\n",
      "Training Loss:  0.0031718339305371046 Validation loss:  0.0024695033207535744 \t num: 1000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.3591496  0.3174811  0.2810474  0.2648716  0.14796233]\n",
      "Y:\t[0.2353786  0.23967282 0.24724782 0.25629455 0.2662226 ]\n",
      "X:\t[0.23270741 0.2353786  0.23967282 0.24724782 0.25629455]\n",
      "Training Loss:  0.0012190474662929773 Validation loss:  0.001719089224934578 \t num: 2000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.50193775 0.52162254 0.5291595  0.543864   0.56907594]\n",
      "Y:\t[0.5281253  0.5462634  0.5647116  0.57908    0.58557963]\n",
      "X:\t[0.5121749 0.5281253 0.5462634 0.5647116 0.57908  ]\n",
      "Training Loss:  0.0010783557081595063 Validation loss:  0.0014228778891265392 \t num: 3000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.48875895 0.49217775 0.50426173 0.5108043  0.518595  ]\n",
      "Y:\t[0.5076821  0.51571226 0.52361387 0.5271411  0.52728444]\n",
      "X:\t[0.5014774  0.5076821  0.51571226 0.52361387 0.5271411 ]\n",
      "Training Loss:  0.0007813406991772354 Validation loss:  0.0013421826297417283 \t num: 4000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.6506153  0.67628807 0.6273282  0.62534034 0.6330676 ]\n",
      "Y:\t[0.6888948  0.6766486  0.66516125 0.6588604  0.6580935 ]\n",
      "X:\t[0.6979587  0.6888948  0.6766486  0.66516125 0.6588604 ]\n",
      "Training Loss:  0.0005473080091178417 Validation loss:  0.0013234427897259593 \t num: 5000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.5090842  0.5077231  0.5027993  0.5007074  0.50631714]\n",
      "Y:\t[0.5324214  0.5300438  0.52998465 0.53465754 0.53975564]\n",
      "X:\t[0.5398769  0.5324214  0.5300438  0.52998465 0.53465754]\n",
      "Training Loss:  0.0006646582041867077 Validation loss:  0.001328767859376967 \t num: 6000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.67104506 0.70919013 0.6957729  0.6653569  0.62591356]\n",
      "Y:\t[0.7326319 0.709704  0.6896337 0.6697501 0.6551345]\n",
      "X:\t[0.7536645 0.7326319 0.709704  0.6896337 0.6697501]\n",
      "Training Loss:  0.0006311979959718883 Validation loss:  0.0010193348862230778 \t num: 7000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.445088   0.49633557 0.545474   0.5858337  0.6425444 ]\n",
      "Y:\t[0.46346575 0.50992495 0.5544495  0.5934921  0.62942755]\n",
      "X:\t[0.4201835  0.46346575 0.50992495 0.5544495  0.5934921 ]\n",
      "Training Loss:  0.0008152222726494074 Validation loss:  0.0010003086645156145 \t num: 8000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.50308317 0.49367124 0.49878457 0.49184498 0.4781111 ]\n",
      "Y:\t[0.48323014 0.47973597 0.47571972 0.47222266 0.4729079 ]\n",
      "X:\t[0.48685142 0.48323014 0.47973597 0.47571972 0.47222266]\n",
      "Training Loss:  0.00110847526229918 Validation loss:  0.0011658224975690246 \t num: 9000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.53554374 0.53824246 0.5308284  0.5215256  0.51424664]\n",
      "Y:\t[0.5193185  0.51422256 0.50985944 0.50430995 0.4965307 ]\n",
      "X:\t[0.52444696 0.5193185  0.51422256 0.50985944 0.50430995]\n",
      "Training Loss:  0.0003495246055535972 Validation loss:  0.0010689778719097376 \t num: 10000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.5879854  0.57784486 0.5709812  0.5640294  0.5526915 ]\n",
      "Y:\t[0.55946404 0.5547408  0.5475955  0.5439742  0.54422   ]\n",
      "X:\t[0.5616878  0.55946404 0.5547408  0.5475955  0.5439742 ]\n",
      "Training Loss:  0.00040659436490386724 Validation loss:  0.00103377690538764 \t num: 11000 \t File: 1\n",
      "Training Progress:\t0.0%\n",
      "P:\t[0.46102375 0.44336492 0.4410593  0.4509782  0.43687138]\n",
      "Y:\t[0.43733978 0.43620685 0.4377648  0.44255906 0.45514977]\n",
      "X:\t[0.44099343 0.43733978 0.43620685 0.4377648  0.44255906]\n",
      "Training Loss:  0.0006696469499729574 Validation loss:  0.0008957135723903775 \t num: 12000 \t File: 1\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    print(\"TRAINING...\")\n",
    "    trained_rnn = train(noizeNet, n_steps,AUDIO_DIR, genreTracks, LSTMBool ,step_size=step_size, duration = duration, numberOfTracks=numberOfTracks, clip=clip)\n",
    "    torch.save(trained_rnn.state_dict(), \"/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/\" + generateModelName(n_steps, print_every, step_size, duration, numberOfTracks, clip, LSTMBool, hidden_dim,n_layers))\n",
    "    duration = seedDuration\n",
    "    # predict(trained_rnn, genreTracks[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, step_size=step_size)\n",
    "    if LSTMBool:\n",
    "        predicted, (hidden, c0) = predict2(trained_rnn, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, c0 =c0)\n",
    "    else: \n",
    "        predicted, hidden = predict2(trained_rnn, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    # instantiate an RNN\n",
    "    noizeNet = NoizeNet(input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob)\n",
    "    noizeNet.load_state_dict(torch.load(\"/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/good model/n_steps=1__print_every=5__step_size=1__duration=10__numberOfTracks=100__clip=5__LSTMBool=Truehidden_dim=200__n_layers=1__lr=0.0001.pt\"))\n",
    "    duration = seedDuration\n",
    "    if LSTMBool:\n",
    "        predicted, (hidden, c0) = predict2(noizeNet, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, c0 =c0)\n",
    "    else: \n",
    "        predicted, hidden = predict2(noizeNet, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
