{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae68bc8",
   "metadata": {},
   "source": [
    "# Noize net\n",
    "The following notebook contains the code used for both an LSTM and RNN used for investigating the feasibility of music generation using an RNN. \n",
    "\n",
    "Note that berevity of code both the RNN and LSTM are implimented in one code base. This does mean that the code is somewhat more complicated and necessitates the use of \"LSTMBool\" which defines throughout the code if operations will take place for an LSTM or RNN. \n",
    "\n",
    "Notes to reader:\n",
    "* This notebook is a refactor of the main python file noizenet.py\n",
    "* This work contains many code iterations and testing which means that there is legacy code scattered throughout the implimentation for different software schemes. Things like batching were tested and then removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284f8e1",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "First we must import all the necessary dependencies.\n",
    "\n",
    "Then check if we can run on a GPU.\n",
    "\n",
    "Additionally we can define the scaler for scaling input data and inverting a scale on output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "035b06ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing on GPU.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib   \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import librosa as lib\n",
    "import os\n",
    "import soundfile as sf #For writing\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import utils\n",
    "import librosa.display\n",
    "import noise\n",
    "\n",
    "\n",
    "# torch.autograd.detect_anomaly(True) #Check for errors and return a stack trace. (Used to debug nan loss)\n",
    "\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "#Print if we are able to use a GPU\n",
    "if(train_on_gpu):\n",
    "    print('Processing on GPU.')\n",
    "else:\n",
    "    print('No GPU available.')\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14e28e",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "Bellow we define some functions that will be used later in this work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1168a0",
   "metadata": {},
   "source": [
    "## Time to FT\n",
    "Convert an array of time/amplitude values to frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ab773b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_fft(time_domain):\n",
    "    # Compute FFT\n",
    "    fft = np.fft.fft(time_domain)\n",
    "    # Concatinate real and imaginary values\n",
    "    new_input = np.concatenate((np.real(fft), np.imag(fft))) #Scheme that takes imaginary into account\n",
    "    #new_input = np.real(fft) #Only consider real values\n",
    "    return new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5785d7",
   "metadata": {},
   "source": [
    "## FT to Time\n",
    "Convert an array of fourier values to the time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1276dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_to_time(ft_domain):\n",
    "    # check if the input can be divinded in two and assign as needed\n",
    "    if ft_domain.shape[0] % 2 == 0:\n",
    "        num_elems = (int)(ft_domain.shape[0] / 2)\n",
    "    else:\n",
    "        ft_domain = ft_domain[0:-1]\n",
    "        num_elems = (int)(ft_domain.shape[0] / 2)\n",
    "    #If we took complex into account\n",
    "    # Get real part \n",
    "    real = ft_domain[0:num_elems]\n",
    "    # Get imaginary part\n",
    "    imag = ft_domain[num_elems:]\n",
    "    # Recompose real and Im parts\n",
    "    composition = real + 1.0j * imag\n",
    "    # ifft back to time domain\n",
    "    \n",
    "    #If we ignore complex values\n",
    "    #composition = ft_domain\n",
    "    time = np.fft.ifft(composition)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f6961",
   "metadata": {},
   "source": [
    "## Batching function \n",
    "This function is rarely used in this work but some testing was done with batches to check their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ba33a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    #Note this code was adapted fome Udacity course on machine learning. \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough elements to fill a batch\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402964b4",
   "metadata": {},
   "source": [
    "## Perlin noise generator\n",
    "For some input array convert each value to perlin noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7555c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genPerlin(x):\n",
    "    tmp = []\n",
    "    for xx in x:\n",
    "        tmp.append(noise.pnoise1(xx)) #Generate new noise value based on single data point\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02a11b",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "Then we can define the Model using the pytorch class definition.\n",
    "\n",
    "The model contains the following layers:\n",
    "* LSM or RNN layer \n",
    "* Dropout Layer\n",
    "* Fully connected layer for translating oyutput\n",
    "\n",
    "After defining the layers in the model we can define a forward function used for the formward pass during training and prediction. \n",
    "* Given c0 (Or None for RNN), the hidden state and some imput x.\n",
    "* Pass the hidden state and input through the RNN layer and recieve some output\n",
    "* Selectively choose values to drop (Note this only happens for hidden layers > 1)\n",
    "* Pass the result through the fully connected layer to obtain the final result\n",
    "* Return the prediction and hidden states\n",
    "\n",
    "Additionally we define a helper funcation to generate the hidden states for an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "72ec3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoizeNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob):\n",
    "        super(NoizeNet, self).__init__()\n",
    "\n",
    "        self.LSTMBool = LSTMBool  #Boolean to determine if the model is RNN or LSTM\n",
    "        self.hidden_dim = hidden_dim #The size of the hidden layer\n",
    "        self.num_layers = n_layers #The number of hidden layers\n",
    "        self.output_size = output_size #The size of the output dimension, always one for this work\n",
    "        self.input_size = input_size #The size of the input dimension, always one for this work\n",
    "        self.dropout_prob = dropout_prob #Prabability for dropout\n",
    "\n",
    "\n",
    "        #If We want an LSTM define LSTM layer and initialise\n",
    "        if LSTMBool:\n",
    "            self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=dropout_prob,batch_first=True)\n",
    "        else:\n",
    "            # define an RNN with specified parameters\n",
    "            self.rnn = nn.RNN(input_size, hidden_dim, n_layers, dropout=dropout_prob, batch_first=True)\n",
    "        \n",
    "        #Define the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # last, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, c0=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            x.cuda()\n",
    "        else:\n",
    "            x.cpu()\n",
    "\n",
    "        if self.LSTMBool:\n",
    "            h_0 = hidden\n",
    "            c_0 = c0\n",
    "            #Rename variables as convention\n",
    "            state = (h_0, c_0)\n",
    "            # Propagate input through LSTM\n",
    "            r_out, (hn, cn) = self.lstm(x, state) #lstm with input, hidden, and internal state\n",
    "            hidden = hn\n",
    "        else:\n",
    "            # get RNN outputs\n",
    "            r_out, hidden = self.rnn(x , hidden)\n",
    "\n",
    "        r_out = self.dropout(r_out) #Dropout\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden.cuda()\n",
    "        \n",
    "        #Reshape output for fully connected layer\n",
    "        r_out = r_out.view(-1, self.hidden_dim)\n",
    "\n",
    "        # get final output\n",
    "        output = self.fc(r_out)\n",
    "        \n",
    "        #Return states based on LSTM bool\n",
    "        if self.LSTMBool:\n",
    "            return output, (hidden, cn)\n",
    "        else:\n",
    "            return output, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        #Initialise the hidden state\n",
    "        if (train_on_gpu):\n",
    "            hidden = torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda() #hidden state\n",
    "            c0 = torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda() #internal state\n",
    "        else:\n",
    "            hidden = torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda() #hidden state\n",
    "            c0 = torch.autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda() #internal state\n",
    "\n",
    "        return hidden, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef2961",
   "metadata": {},
   "source": [
    "# Training\n",
    "We can then define the training function. This function takes in the model, n_steps which is a bit of a misnoma as it is used as the number of full \"frame\" steps that will complete the impute sequence. An array of genreTracks which are translated into file names. Using AUDIO_DIR and file names we can read in the input songs. Step size is used to determine how large of a step to take in the training data. Duration is the duration of the input song to read in. Number of tracks is the number of songs that we will train on. Clip is the value of the griadient clipping. \n",
    "\n",
    "In this work we vary the training procedue among trails. However the procedure remains unchanged and is as follows:\n",
    "* Ensure model is in training mode\n",
    "* load validation data and perform and needed transformations/scaling\n",
    "* Initialize hidden layers\n",
    "* Loop over a number of input tracks\n",
    "* At each loop iteration\n",
    "    * Load and transform input training data\n",
    "    * Define the needed variables\n",
    "    * Check if input data contains a NaN value\n",
    "    * For some number of steps that was calculated earlier\n",
    "        * For some number of epochs \n",
    "            * Select some data\n",
    "            * Ensure data has a shift in input and target\n",
    "            * Convert to tensors and give the input a batch size dimension of one\n",
    "            * Move data to GPU if we can\n",
    "            * Get a prediction frome the LSTM or RNN\n",
    "            * Detach hidden states from history\n",
    "            * Zero gradients\n",
    "            * Calculate loss and add it to the plot array\n",
    "            * Perform backward step\n",
    "            * Clip gradients\n",
    "            * Take an optimizer step to optimize weights\n",
    "            * Perform validation (I will not walk through this as it is the same procedure in eval mode)\n",
    "    * Clean memory if the model is large\n",
    "* Plot losses\n",
    "* Return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4d0b4692",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train(noizeNet, n_steps, AUDIO_DIR, genreTracks, LSTM ,step_size=1, duration=5, numberOfTracks=1, clip=5, fft_bool=False):\n",
    "    fileCount = 0 #Used for displaying the file that is currently being trained on\n",
    "    noizeNet.train() #Set the model to training mode\n",
    "    lossArr = [] #Array used to plot loss over time\n",
    "    val_losses = [] #Array used to store validation losses\n",
    "    if(train_on_gpu):\n",
    "        noizeNet.cuda() #Move the model to GPU if available\n",
    "\n",
    "    val_file = utils.get_audio_path(AUDIO_DIR, genreTracks[-1]) #Get file name of validation data\n",
    "    val_data, sr = lib.load(val_file, mono=True, duration = duration) #Load validation data\n",
    "    \n",
    "    \n",
    "    if fft_bool:\n",
    "        print(\"We are training on FFT data\")\n",
    "        val_data = time_to_fft(val_data) #FFT validation data\n",
    "    val_data = scaler.fit_transform(val_data.reshape(-1,1)) #Scale data\n",
    "    \n",
    "\n",
    "    \n",
    "    #Loop over all the files in our filtered list\n",
    "    for id in genreTracks: \n",
    "        #Stop training after n files\n",
    "        fileCount+=1\n",
    "        if(fileCount > numberOfTracks):\n",
    "            break \n",
    "\n",
    "        filename = utils.get_audio_path(AUDIO_DIR, id) #Get the actual path to the file from the id\n",
    "        \n",
    "        fileData, sr = lib.load(filename, mono=True, duration = duration) #Load training song\n",
    "        fileData = fileData.reshape(-1,1) #Convert to correct shape\n",
    "        \n",
    "        if fft_bool:\n",
    "            fileData = time_to_fft(fileData) #If we want to train on FFT data\n",
    "        \n",
    "        fileData = scaler.fit_transform(fileData) #Scale the data\n",
    "\n",
    "        batch_size = (int)(duration*sr/n_steps) #Find the size of the window that slides across the input song\n",
    "        number_of_steps = (int)(duration*sr)-batch_size #Assumes step size of one as a larger step size produced poor results\n",
    "        if(number_of_steps == 0):\n",
    "            number_of_steps = 1\n",
    "        if(np.isnan(np.sum(fileData))):\n",
    "            print(\"NAN ON FILE:\\t\", filename)\n",
    "            break\n",
    "        for e in range(0,100):\n",
    "            if LSTM:\n",
    "                    (hidden, c0) = noizeNet.init_hidden(1)\n",
    "                else:\n",
    "                    hidden = None\n",
    "            for batch_i in (range(0,number_of_steps, step_size)):\n",
    "                \n",
    "                # for x, y in get_batches(fileData, 500, 50): #Note this was used in the testing of batched data\n",
    "                \n",
    "                # defining the training data\n",
    "                data = fileData[batch_i: batch_size + batch_i]\n",
    "                data = data.reshape(-1,1)   \n",
    "                x = data[:-1] #Select all but the last element in the input data\n",
    "                y = data[1:] #Select all but the first element in the input data. Essentially a forward shift in time\n",
    "\n",
    "                # convert data into Tensors\n",
    "                x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "                y_tensor = torch.Tensor(y)\n",
    "                if(train_on_gpu):\n",
    "                        x_tensor, y_tensor = x_tensor.cuda(), y_tensor.cuda() #Move tensors to GPU\n",
    "                \n",
    "                if LSTM:\n",
    "                    prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0) #Get output from LSTM\n",
    "                else:\n",
    "                    prediction, hidden = noizeNet(x_tensor, hidden) #Get output from RNN\n",
    "                \n",
    "                #Detach hidden state from history\n",
    "                hidden = hidden.data\n",
    "\n",
    "                if LSTM:\n",
    "                    c0 = c0.data #Detatch cell state from history\n",
    "\n",
    "\n",
    "                # zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # calculate the loss\n",
    "                \n",
    "                loss = criterion(prediction, y_tensor)\n",
    "                # if(np.isnan(loss)):\n",
    "                #     break\n",
    "                lossArr.append(loss.item())\n",
    "                # perform backprop and update weights\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_value_(noizeNet.parameters(), clip) #Clip gradient\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "                #Validation step. I won't comment the case as the logic is same as above. Note we are in eval mode though!\n",
    "                if(int((batch_i)) % 1000 == 0):\n",
    "                    if LSTM:\n",
    "                        (val_h, val_c) = noizeNet.init_hidden(1)\n",
    "                    else:\n",
    "                        val_h = None\n",
    "\n",
    "                    \n",
    "                    noizeNet.eval()\n",
    "                    val_x = val_data[:-1]\n",
    "                    val_y = val_data[1:]\n",
    "\n",
    "                    \n",
    "\n",
    "                    for ep in range(0,10):\n",
    "                        val_x = val_data[:-1]\n",
    "                        val_y = val_data[1:]\n",
    "                        val_x, val_y = torch.from_numpy(val_x), torch.from_numpy(val_y)\n",
    "                        \n",
    "                        \n",
    "                        inputs, targets = val_x.reshape(-1,1).unsqueeze(0), val_y.reshape(-1,1)\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        if LSTM:\n",
    "                            output, (val_h, val_c) = noizeNet(inputs, val_h, val_c)\n",
    "                        else:\n",
    "                            output, val_h = noizeNet(inputs, val_h)\n",
    "                        val_loss  = criterion(output, targets)\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                        if LSTM:\n",
    "                            val_h = val_h.data\n",
    "                            val_c = val_c.data\n",
    "                        else:\n",
    "                            val_h = val_h.data\n",
    "\n",
    "\n",
    "                    print(\"Training Progress:\\t\", round((fileCount*e/(number_of_steps*numberOfTracks*100))*100, 2), \"%\"  , sep=\"\")\n",
    "                    print(\"P:\\t\", prediction.cpu().data.numpy().flatten()[-5:],\"\\nY:\\t\", y[-5:].flatten() ,\"\\nX:\\t\" , x[-5:].flatten() ,sep=\"\")\n",
    "                    print('Training Loss: ', loss.item(), \"Validation loss: \", val_loss.item() , \"\\t num:\", batch_i, \"\\t File:\", fileCount)\n",
    "                    noizeNet.train() # reset to train mode after validation\n",
    "            \n",
    "        #Clean unused variables to ensure memory is kept free\n",
    "        del prediction\n",
    "        del x_tensor\n",
    "        del y_tensor\n",
    "        del data\n",
    "        del x\n",
    "        del y\n",
    "        gc.collect()\n",
    "    #Plot validation loss and training loss\n",
    "    fig, ax = plt.subplots(nrows=2)\n",
    "    ax[0].plot(lossArr)\n",
    "    ax[0].set(title=\"Training loss\", ylabel=\"Loss\", xlabel=\"Epochs\")\n",
    "    ax[1].plot(val_losses)\n",
    "    ax[1].set(title=\"Validation loss\", ylabel=\"Loss\", xlabel=\"Epochs\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return noizeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f39f4",
   "metadata": {},
   "source": [
    "# Prediction and Music generation\n",
    "\n",
    "Then we define the prediction function. \n",
    "\n",
    "In this work we have two prediction schemes that produce equivalent results. I will explain them independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f83578",
   "metadata": {},
   "source": [
    "## Prediction scheme 1 \n",
    "This prediction scheme itteratively runs through the sequence of input seed values and creates a prediction which is appended to the seed. This new sequence is used as a seed for the next prediction. Here we attempt to replace any knowledge of the old seed with purely knowledge from predicted values. \n",
    "\n",
    "This prediction sheme functions as follows\n",
    "* Recieve the model, prediction seed track, duration of seed, prediction duration and LSTM bool\n",
    "* Read in the seed data and perform any needed scaling/transformation\n",
    "* Select some range of the seed and obtain a prediction from the model\n",
    "* Record the last element in the prediction\n",
    "* Append this predicted value to a music array that will become the generated song\n",
    "* Shift the data window to exclude an early data point and append the predicted value\n",
    "* Repeat this procedure until we have predicted enough points for the desired music output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c661a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(noizeNet, genreTrack ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1, fft_bool=False):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval() #move to eval mode\n",
    "    hidden = None #instantiate hidden\n",
    "    c0 = None #Instantiate c0\n",
    "    if LSTMBool:\n",
    "        (hidden, c0) = noizeNet.init_hidden(1) #initialize hidden states\n",
    "        \n",
    "    filePath = utils.get_audio_path(AUDIO_DIR, genreTrack) #Get the actual path to the file from the id\n",
    "    y, sr = lib.load(filePath, mono=True, duration = duration) #Read the input seed\n",
    "    if fft_bool:\n",
    "        y = time_to_fft(y) #Move to frequency domain if needed\n",
    "    y = scaler.fit_transform(y.reshape(-1,1)) #Scale seed\n",
    "\n",
    "    data = y\n",
    "\n",
    "    # data = np.random.normal(-1,1,y.shape) #Generate normal noise\n",
    "    # data = genPerlin(np.linspace(0,1,y.size)) #Generate perlin noise\n",
    "    batch_size = (int)(sr*duration)\n",
    "\n",
    "    number_of_steps = (int)(sr*predictDuration/step_size) #Calculate the number of steps\n",
    "\n",
    "    music = [] #Instantiate the music array\n",
    "    next = data[batch_size-1] #Find the \"next value\"\n",
    "\n",
    "    sf.write('predictionSeed.wav', np.append( data[step_size: batch_size], next), sr,format=\"WAV\") #Wite the seed\n",
    "    print(\"BATCH SIZE:\", batch_size ,sep=\"\\t\")\n",
    "    print(\"NUMBER OF STEPS:\", number_of_steps , sep=\"\\t\")\n",
    "\n",
    "    \n",
    "    for batch_i in (range(0, number_of_steps)):\n",
    "        if LSTMBool:\n",
    "            hidden, c0 = noizeNet.init_hidden(1) #init hidden states\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda() #Move to GPU\n",
    "        data = data[batch_i: batch_size-1 + batch_i] #Select piece of data\n",
    "        data = np.append(data,next) #Append data to music\n",
    "        x = data.reshape(-1,1) #Ensure correct vector shape\n",
    "        print(x.shape)\n",
    "        if(np.isnan(np.sum(data))):\n",
    "            print(\"data contains NAN\", data) #Check if input seed has a NaN value\n",
    "\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda() #Move to GPU if possible\n",
    "\n",
    "        if(LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0) #Get prediction from LSTM\n",
    "            c0 = c0.data #detach cell state from history\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden) #Get RNN prediction\n",
    "        hidden = hidden.data #detach hidden state from history\n",
    "        \n",
    "        \n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-1]) #Append predicted value to music\n",
    "        next = prediction.cpu().data.numpy().flatten()[-1] #Set next to the predicted value\n",
    "        \n",
    "        #Print progress\n",
    "        if(int((batch_i)) % 1000 == 0):\n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    print(music)\n",
    "    music = scaler.inverse_transform(music.reshape(-1,1)) #Invert scale\n",
    "    if fft_bool:\n",
    "        music = fft_to_time(music).astype('float32') #Move to time domain if needed\n",
    "    \n",
    "    #Write the song\n",
    "    sf.write('/home/liam/Desktop/University/2021/MAM3040W/thesis/writeup/code/outputSoundFile.wav', (music), 22050,format=\"WAV\")\n",
    "    time_steps = np.linspace(0, len(music), len(music))\n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=0.1) #Plot predicted result\n",
    "    plt.show()\n",
    "    return music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc0820",
   "metadata": {},
   "source": [
    "## Prediction Scheme 2\n",
    "This scheme is similar to the previous one, however, more efficient as we process the seed element by elements and then pass the hidden states to a new function that will begin predicting new values from the last predicted value from the seed and hidden states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206327fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(noizeNet, genreTrack ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1, fft_bool=False):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval() #Ensure we are in eval mode\n",
    "\n",
    "    if LSTMBool:\n",
    "        hidden, c0 = noizeNet.init_hidden(1) #init hidden state and cell state\n",
    "    else:\n",
    "        hidden = None\n",
    "    filePath = utils.get_audio_path(AUDIO_DIR, genreTrack) #Get the actual path to the file from the id\n",
    "    y, sr = lib.load(filePath, mono=True, duration = duration) # Load seed data\n",
    "    if fft_bool:\n",
    "        y = time_to_fft(y) #Move to frequency domain if needed\n",
    "    y = scaler.fit_transform(y.reshape(-1,1)) #Scale data\n",
    "#     y = np.random.normal(-1,1,y.shape) #Generate normal noise as seed\n",
    "#     data = genPerlin(np.linspace(0,1,y.size)) #Generate perlin noise as seed\n",
    "    sf.write('Predict2Seed.wav', y, 22050,format=\"WAV\") #Write the seed\n",
    "    \n",
    "\n",
    "    number_of_steps = (int)(sr*duration/step_size) #Calculate the number of steps needed to process the seed\n",
    "    music = [] #Initialise a list for the predicted music \n",
    "    for batch_i in (range(0, number_of_steps)): #Loop over the number of steps required\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda() #Move to GPU if we can\n",
    "        data = y[batch_i] #Get the current prediction seed value\n",
    "        data = data.reshape(-1,1) #Ensure the correct shape\n",
    "        x = data \n",
    "\n",
    "\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda() #Move to GPU if we can\n",
    "\n",
    "        if(LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0) #Get the LSTM prediction\n",
    "            c0 = c0.data #Detach cell state from history\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden) #Get the RNN prediction\n",
    "        hidden = hidden.data #Detach hidden state from history\n",
    "        \n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-step_size:]) #Append predicted value to music\n",
    "        #Print progress\n",
    "        if(int((batch_i)) % 1000 == 0):\n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    sf.write('outputSoundFilePredict2.wav', (music), 22050,format=\"WAV\") #Write the reproduction\n",
    "    time_steps = np.linspace(0, len(music), len(music)) \n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=1) #Plot the reproduction\n",
    "    plt.show()\n",
    "    if LSTMBool:\n",
    "        return music, (hidden, c0) #Return hidden states and prediction\n",
    "    else: \n",
    "        return music, hidden #Return hidden state and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87148202",
   "metadata": {},
   "source": [
    "## Predict 3\n",
    "This function, uses the hidden states from predict 2 to begin generating new music predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d657fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict3(noizeNet, seeded ,duration=1, n_steps=30, LSTMBool=False, predictDuration = 30, step_size=1, hidden = None, c0 = None, fft_bool=False):\n",
    "    print(\"PREDICTING...\")\n",
    "    noizeNet.eval() #Ensure we are in eval mode\n",
    "    number_of_steps = (int)(22050*predictDuration/step_size) #Calculate the number of predictions needed\n",
    "    music = [] #Instantiate the list used for sotring the generated music\n",
    "    x = [0] #Give x a value. This is only needed for noise injection (perturation)\n",
    "    for batch_i in (range(0, number_of_steps)): #Loop for the number of predicted steps needed\n",
    "        if(train_on_gpu):\n",
    "            noizeNet.cuda()\n",
    "        seeded = np.array([seeded]).reshape(-1,1) #Ensure vector is in the correct shape\n",
    "        data = seeded\n",
    "#         if x[0] - seeded[0] < 0.0001:\n",
    "#             data = seeded + np.random.normal(-1,1)\n",
    "        data = data.reshape(-1,1) #Ensure vector is in the correct shape\n",
    "        x = data\n",
    "        \n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0)  # unsqueeze gives a 1, batch_size dimension\n",
    "\n",
    "        if(train_on_gpu):\n",
    "                x_tensor = x_tensor.cuda()\n",
    "\n",
    "        if (LSTMBool):\n",
    "            prediction, (hidden, c0) = noizeNet(x_tensor, hidden, c0)  #Obtain prediction\n",
    "            c0 = c0.data  #Detach cell state from history\n",
    "        else:\n",
    "            prediction, hidden = noizeNet(x_tensor, hidden) #Obtain RNN prediction\n",
    "        hidden = hidden.data # Detach hidden state from history\n",
    "        \n",
    "        seeded = prediction.cpu().data.numpy()[-1:step_size:] #Get the next value for prediction \n",
    "        music = np.append(music,(prediction.cpu().data.numpy().flatten())[-step_size:]) #Append predicted value to music\n",
    "        #Print progress\n",
    "        if(int((batch_i)) % 100 == 0):\n",
    "            print(\"PROGRESS:\\t\", round(((batch_i)/number_of_steps)*100, 2), \"%\"  , sep=\"\")\n",
    "            print(\"Prediction dimensions:\\t\", prediction.cpu().size(), \"\\t\" ,prediction.cpu().data.numpy().flatten().size, \"\\nMusic dimensions:\\t\", music.size ,sep=\"\")\n",
    "       \n",
    "    \n",
    "    music = scaler.inverse_transform(music.reshape(-1,1)) #Invert the scaling\n",
    "    if fft_bool:\n",
    "        music = fft_to_time(music).astype('float32') #Move back to time domain if needed\n",
    "    \n",
    "    print(music)\n",
    "    sf.write('outputSoundFilePrediction3.wav', (music), 22050,format=\"WAV\") #Write generated music\n",
    "    time_steps = np.linspace(0, len(music), len(music))\n",
    "    plt.plot(time_steps, music,\"b.\",  markersize=1) #Plot generated music sample view\n",
    "    plt.show()\n",
    "    return music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fa53b",
   "metadata": {},
   "source": [
    "## Saving function\n",
    "A helper function used for naming a model by it's hyer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModelName(n_steps = 30, print_every = 5, step_size =  1, duration = 5, numberOfTracks = 1, clip = 5, LSTMBool=False, hidden_dim=50,n_layers=1, fft_bool=False):\n",
    "    return \"n_steps=\"+ str(n_steps) + \"__\" +\"print_every=\"+ str(print_every) + \"__\" +\"step_size=\"+ str(step_size) + \"__\" +\"duration=\"+ str(duration) + \"__\" +  \\\n",
    "    \"numberOfTracks=\"+ str(numberOfTracks) + \"__\" +  \"clip=\"+ str(clip) + \"__\" +  \"LSTMBool=\"+ str(LSTMBool) + \"hidden_dim=\"+ str(hidden_dim) + \"__\" +\"n_layers=\"+ str(n_layers) +\"__\"+ \"lr=\" + str(lr) + \"__fft_bool\" + str(fft_bool)+ \".pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10314be9",
   "metadata": {},
   "source": [
    "# Hyper parameters\n",
    "Before we start training we must declair the hyperparameters and instantiate the model.\n",
    "\n",
    "I believve all of these variables should be self explanatory. Note this is also where we choose if the miodel will be an LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on hyperparameters\n",
    "input_size=1\n",
    "output_size=1\n",
    "hidden_dim=30 \n",
    "n_layers=1\n",
    "LSTMBool = True\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# instantiate an RNN/LSTM\n",
    "noizeNet = NoizeNet(input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob)\n",
    "print(noizeNet)\n",
    "\n",
    "# The two criterions used in this work\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()\n",
    "lr=0.00001\n",
    "\n",
    "# The various optimizers used in this work\n",
    "optimizer = torch.optim.Adam(noizeNet.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(noizeNet.parameters(),lr=lr)\n",
    "# optimizer = torch.optim.Adadelta(noizeNet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0fe95",
   "metadata": {},
   "source": [
    "# Music filtering by genre\n",
    "\n",
    "Here we just filter all the data in FMA for some chosen genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get metadata for fma dataset\n",
    "AUDIO_DIR = \"data/fma_small/\"\n",
    "\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv') #Load metadata\n",
    "\n",
    "small = tracks['set', 'subset'] <= 'small' #select the FMA small dataset\n",
    "genre1 = tracks['track', 'genre_top'] == 'Instrumental' #Select only instrumental songs\n",
    "# genre2 = tracks['track', 'genre_top'] == 'Hip-Hop' #We can set multilpe genres bellow as (genre1 | genre2)\n",
    "genreTracks = list(tracks.loc[small & (genre1),('track', 'genre_top')].index) #Get a list of filtered songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddefd4",
   "metadata": {},
   "source": [
    "# Training variables\n",
    "\n",
    "Initialise the variables needed for training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set if we want to train new model or load and predict with saved model\n",
    "TRAIN = True\n",
    "\n",
    "fft_bool = False\n",
    "n_steps = 1 #The number of full frame steps to be taken to complete training\n",
    "print_every = 5 #Not actually used. This was replaced by persentage printing\n",
    "step_size =  1 #The step size taken by each training frame \n",
    "duration = 0.1 #The duration of the training segment\n",
    "predictDuration = 0.1 #The duration of the predicted song is seconds\n",
    "numberOfTracks = 2 #The number of tracks to be trained on\n",
    "clip = 1 #Gradient clipping\n",
    "seedDuration = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec44f7f",
   "metadata": {},
   "source": [
    "# Begin Training or load model and predict\n",
    "Then we can call our training and prediction functions as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print(\"Training...\")\n",
    "    trained_rnn = train(noizeNet, n_steps,AUDIO_DIR, genreTracks, LSTMBool ,step_size=step_size, duration = duration, numberOfTracks=numberOfTracks, clip=clip)\n",
    "    torch.save(trained_rnn.state_dict(), generateModelName(n_steps, print_every, step_size, duration, numberOfTracks, clip, LSTMBool, hidden_dim,n_layers, fft_bool))\n",
    "    duration = seedDuration\n",
    "    predict(trained_rnn, genreTracks[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, step_size=step_size)\n",
    "    if LSTMBool:\n",
    "        predicted, (hidden, c0) = predict2(trained_rnn, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, fft_bool=fft_bool)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, c0 =c0, fft_bool=fft_bool)\n",
    "    else: \n",
    "        predicted, hidden = predict2(trained_rnn, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, fft_bool=fft_bool)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, fft_bool=fft_bool)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    # instantiate an model\n",
    "    noizeNet = NoizeNet(input_size, output_size, hidden_dim, n_layers, LSTMBool, dropout_prob)\n",
    "    #Load saved model\n",
    "    noizeNet.load_state_dict(torch.load(\"n_steps=1__print_every=5__step_size=1__duration=10__numberOfTracks=100__clip=5__LSTMBool=Truehidden_dim=200__n_layers=1__lr=0.0001.pt\"))\n",
    "    duration = seedDuration #Set duration = seed duration for prediction seed\n",
    "    predict(trained_rnn, genreTracks[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, step_size=step_size)\n",
    "    if LSTMBool:\n",
    "        predicted, (hidden, c0) = predict2(noizeNet, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, fft_bool=fft_bool)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, c0 =c0)\n",
    "    else: \n",
    "        predicted, hidden = predict2(noizeNet, genreTracks[-2] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration, fft_bool=fft_bool)\n",
    "        predict3(noizeNet, predicted[-1] ,duration=duration, n_steps=n_steps, LSTMBool=LSTMBool, predictDuration = predictDuration,  hidden=hidden, fft_bool=fft_bool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
