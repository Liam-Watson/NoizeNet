\BOOKMARK [1][-]{section.1}{Abstract}{}% 1
\BOOKMARK [1][-]{section.2}{Introduction}{}% 2
\BOOKMARK [1][-]{section.3}{Introduction to Neural networks}{}% 3
\BOOKMARK [2][-]{subsection.3.1}{Perceptron}{section.3}% 4
\BOOKMARK [2][-]{subsection.3.2}{Multi layer perceptron network}{section.3}% 5
\BOOKMARK [2][-]{subsection.3.3}{Activation functions}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.4}{Feed forward}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.5}{Error functions \(Loss functions\)}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.6}{Gradient decent}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.7}{Back propagation}{section.3}% 10
\BOOKMARK [1][-]{section.4}{Introduction to Recurrent Neural Networks}{}% 11
\BOOKMARK [2][-]{subsection.4.1}{RNN concepts}{section.4}% 12
\BOOKMARK [2][-]{subsection.4.2}{LSTM}{section.4}% 13
\BOOKMARK [3][-]{subsubsection.4.2.1}{The Learn Gate \(Input gate\)}{subsection.4.2}% 14
\BOOKMARK [3][-]{subsubsection.4.2.2}{The Forget Gate}{subsection.4.2}% 15
\BOOKMARK [3][-]{subsubsection.4.2.3}{The Remember Gate \(Cell State\)}{subsection.4.2}% 16
\BOOKMARK [3][-]{subsubsection.4.2.4}{The Use Gate \(The output gate\) }{subsection.4.2}% 17
\BOOKMARK [3][-]{subsubsection.4.2.5}{Rigorous representation of LSTM operations}{subsection.4.2}% 18
\BOOKMARK [1][-]{section.5}{NoizeNet}{}% 19
\BOOKMARK [2][-]{subsection.5.1}{Data and pre-processing}{section.5}% 20
\BOOKMARK [3][-]{subsubsection.5.1.1}{Digital Music}{subsection.5.1}% 21
\BOOKMARK [3][-]{subsubsection.5.1.2}{An introduction to digital sound representations}{subsection.5.1}% 22
\BOOKMARK [3][-]{subsubsection.5.1.3}{Additional data representation}{subsection.5.1}% 23
\BOOKMARK [3][-]{subsubsection.5.1.4}{Data used for generation seed}{subsection.5.1}% 24
\BOOKMARK [3][-]{subsubsection.5.1.5}{Data scaling}{subsection.5.1}% 25
\BOOKMARK [2][-]{subsection.5.2}{Architecture of NoizeNet}{section.5}% 26
\BOOKMARK [2][-]{subsection.5.3}{Implementation}{section.5}% 27
\BOOKMARK [3][-]{subsubsection.5.3.1}{Hyperparameters}{subsection.5.3}% 28
\BOOKMARK [2][-]{subsection.5.4}{A note on numerical stability of the model}{section.5}% 29
\BOOKMARK [3][-]{subsubsection.5.4.1}{A note on batching}{subsection.5.4}% 30
\BOOKMARK [2][-]{subsection.5.5}{Results}{section.5}% 31
\BOOKMARK [3][-]{subsubsection.5.5.1}{Training and validation}{subsection.5.5}% 32
\BOOKMARK [3][-]{subsubsection.5.5.2}{Initial investigation}{subsection.5.5}% 33
\BOOKMARK [3][-]{subsubsection.5.5.3}{RNN and LSTM comparison}{subsection.5.5}% 34
\BOOKMARK [2][-]{subsection.5.6}{Addressing prediction convergence}{section.5}% 35
\BOOKMARK [2][-]{subsection.5.7}{Introducing artificial noise}{section.5}% 36
\BOOKMARK [2][-]{subsection.5.8}{Predicting from noise}{section.5}% 37
\BOOKMARK [2][-]{subsection.5.9}{Fourier Transform results}{section.5}% 38
\BOOKMARK [1][-]{section.6}{Discussion}{}% 39
\BOOKMARK [2][-]{subsection.6.1}{Computational complexity}{section.6}% 40
\BOOKMARK [2][-]{subsection.6.2}{Recommendations for further investigation}{section.6}% 41
\BOOKMARK [1][-]{section.7}{Conclusion}{}% 42
